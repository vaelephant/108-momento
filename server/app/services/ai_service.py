"""
AIæœåŠ¡å±‚ - å¤„ç†å›¾åƒè¯†åˆ«ã€æ ‡ç­¾ç”Ÿæˆç­‰AIä»»åŠ¡
"""
import os
import cv2
import numpy as np
from PIL import Image
from typing import List, Dict, Any, Optional
import torch
from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    CLIPProcessor, CLIPModel
)
import logging

from app.config import settings
from app.core.exceptions import AIProcessingError

logger = logging.getLogger(__name__)


class AIService:
    """AIæœåŠ¡ç±»"""
    
    def __init__(self):
        self.device = self._get_device()
        self.models = {}
        self._load_models()
    
    def _get_device(self) -> str:
        """è·å–è®¡ç®—è®¾å¤‡"""
        if settings.device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return settings.device
    
    def _load_models(self):
        """åŠ è½½AIæ¨¡å‹"""
        try:
            # åŠ è½½BLIP-2æ¨¡å‹ç”¨äºå›¾åƒæè¿°
            self.models['blip_processor'] = BlipProcessor.from_pretrained(
                "Salesforce/blip-image-captioning-base"
            )
            self.models['blip_model'] = BlipForConditionalGeneration.from_pretrained(
                "Salesforce/blip-image-captioning-base"
            ).to(self.device)
            
            # åŠ è½½CLIPæ¨¡å‹ç”¨äºå›¾åƒ-æ–‡æœ¬åŒ¹é…
            self.models['clip_processor'] = CLIPProcessor.from_pretrained(
                "openai/clip-vit-base-patch32"
            )
            self.models['clip_model'] = CLIPModel.from_pretrained(
                "openai/clip-vit-base-patch32"
            ).to(self.device)
            
            logger.info("AIæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise AIProcessingError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def process_photo_simple(self, photo_id: int, image_path: str):
        """ç®€å•çš„åå°å¤„ç†ç…§ç‰‡ - ä½¿ç”¨æ··åˆAIæœåŠ¡"""
        try:
            logger.info(f"ğŸš€ [AI] å¼€å§‹å¤„ç†ç…§ç‰‡ {photo_id}")
            
            # ä½¿ç”¨æ··åˆAIæœåŠ¡
            from app.services.hybrid_ai_service import hybrid_ai_service
            result = hybrid_ai_service.process_photo_simple(photo_id, image_path)
            
            logger.info(f"âœ… [AI] ç…§ç‰‡ {photo_id} å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"AIå¤„ç†å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
    
    def _process_photo_sync(self, photo_id: int, image_path: str):
        """åŒæ­¥å¤„ç†ç…§ç‰‡"""
        try:
            # åŠ è½½å›¾åƒ
            image = self._load_image(image_path)
            if image is None:
                raise AIProcessingError("å›¾åƒåŠ è½½å¤±è´¥")
            
            # ç”Ÿæˆå›¾åƒæè¿°
            caption = self._generate_caption(image)
            
            # æå–ä¸»è‰²è°ƒ
            dominant_colors = self._extract_dominant_colors(image)
            
            # ç”Ÿæˆæ ‡ç­¾
            tags = self._generate_tags(image)
            
            # ç”Ÿæˆå›¾åƒåµŒå…¥
            embedding = self._generate_embedding(image)
            
            # è¿™é‡Œåº”è¯¥æ›´æ–°æ•°æ®åº“
            # æš‚æ—¶æ‰“å°ç»“æœ
            logger.info(f"ç…§ç‰‡ {photo_id} å¤„ç†å®Œæˆ:")
            logger.info(f"  æè¿°: {caption}")
            logger.info(f"  ä¸»è‰²è°ƒ: {dominant_colors}")
            logger.info(f"  æ ‡ç­¾: {tags}")
            
        except Exception as e:
            logger.error(f"ç…§ç‰‡å¤„ç†å¤±è´¥: {e}")
            raise AIProcessingError(f"ç…§ç‰‡å¤„ç†å¤±è´¥: {e}")
    
    def _load_image(self, image_path: str) -> Optional[np.ndarray]:
        """åŠ è½½å›¾åƒ"""
        try:
            if not os.path.exists(image_path):
                return None
            
            # ä½¿ç”¨PILåŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            image_array = np.array(image)
            
            return image_array
            
        except Exception as e:
            logger.error(f"å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return None
    
    def _generate_caption(self, image: np.ndarray) -> str:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        try:
            # è½¬æ¢å›¾åƒæ ¼å¼
            pil_image = Image.fromarray(image)
            
            # ä½¿ç”¨BLIPç”Ÿæˆæè¿°
            inputs = self.models['blip_processor'](pil_image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                out = self.models['blip_model'].generate(**inputs, max_length=50)
            
            caption = self.models['blip_processor'].decode(out[0], skip_special_tokens=True)
            
            return caption
            
        except Exception as e:
            logger.error(f"æè¿°ç”Ÿæˆå¤±è´¥: {e}")
            return "æ— æ³•ç”Ÿæˆæè¿°"
    
    def _extract_dominant_colors(self, image: np.ndarray, k: int = 5) -> List[str]:
        """æå–ä¸»è‰²è°ƒ"""
        try:
            # é‡å¡‘å›¾åƒæ•°æ®
            data = image.reshape((-1, 3))
            data = np.float32(data)
            
            # ä½¿ç”¨K-meansèšç±»
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            
            # è½¬æ¢é¢œè‰²æ ¼å¼
            colors = []
            for center in centers:
                r, g, b = center.astype(int)
                hex_color = f"#{r:02x}{g:02x}{b:02x}"
                colors.append(hex_color)
            
            return colors
            
        except Exception as e:
            logger.error(f"é¢œè‰²æå–å¤±è´¥: {e}")
            return []
    
    def _generate_tags(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ ‡ç­¾"""
        try:
            # é¢„å®šä¹‰çš„æ ‡ç­¾å€™é€‰
            tag_candidates = [
                "cat", "dog", "car", "tree", "building", "person", "food", "nature",
                "sky", "water", "mountain", "beach", "city", "street", "indoor", "outdoor"
            ]
            
            # è½¬æ¢å›¾åƒæ ¼å¼
            pil_image = Image.fromarray(image)
            
            # ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒ-æ–‡æœ¬åŒ¹é…
            inputs = self.models['clip_processor'](
                text=tag_candidates,
                images=pil_image,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.models['clip_model'](**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
            
            # æå–é«˜ç½®ä¿¡åº¦çš„æ ‡ç­¾
            tags = []
            for i, prob in enumerate(probs[0]):
                if prob > 0.1:  # ç½®ä¿¡åº¦é˜ˆå€¼
                    tags.append({
                        "name": tag_candidates[i],
                        "confidence": float(prob),
                        "source": "clip"
                    })
            
            return tags
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _generate_embedding(self, image: np.ndarray) -> Optional[List[float]]:
        """ç”Ÿæˆå›¾åƒåµŒå…¥å‘é‡"""
        try:
            # è½¬æ¢å›¾åƒæ ¼å¼
            pil_image = Image.fromarray(image)
            
            # ä½¿ç”¨CLIPç”ŸæˆåµŒå…¥
            inputs = self.models['clip_processor'](images=pil_image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                image_features = self.models['clip_model'].get_image_features(**inputs)
                # å½’ä¸€åŒ–
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            return image_features.cpu().numpy().tolist()[0]
            
        except Exception as e:
            logger.error(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def find_similar_images(self, query_image_path: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸ä¼¼å›¾åƒ"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å›¾åƒçš„åµŒå…¥
            query_image = self._load_image(query_image_path)
            if query_image is None:
                return []
            
            query_embedding = self._generate_embedding(query_image)
            if query_embedding is None:
                return []
            
            # è¿™é‡Œåº”è¯¥ä¸æ•°æ®åº“ä¸­çš„åµŒå…¥è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
            # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
            return []
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼å›¾åƒæœç´¢å¤±è´¥: {e}")
            return []
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "device": self.device,
            "models_loaded": list(self.models.keys()),
            "cuda_available": torch.cuda.is_available(),
            "mps_available": torch.backends.mps.is_available()
        }
